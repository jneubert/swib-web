---
pagetitle: SWIB23 Programme
---

<div id="main">

::: hint
[Day 1](#day1) &#160; [Day 2](#day2) &#160; [Day 3](#day3) &#160; 
:::

# Programme


<div id="day1">

## DAY 1 | Monday, 2023-09-11

<table>

<tr id="P1"><td class="times-col">10:00–12:15</td><td>
### Satellite Event




</td></tr>


<tr id="contrib134"><td></td><td>

#### VIVO meetup


Christian Hauschke [<img src="images/orcid.png" title="ORCID: 0000-0003-2499-7741">](https://orcid.org/0000-0003-2499-7741)<sup>1</sup>, Jose Francisco Salm Junior [<img src="images/orcid.png" title="ORCID: 0000-0002-8492-1645">](https://orcid.org/0000-0002-8492-1645)<sup>2</sup>\
<sup>1 </sup>Technische Informationsbibliothek (TIB), Germany; <sup>2 </sup>Universidade Federal de Santa Catarina, Brazil


VIVO is a linked data based research information system. This collocated event serves as a user meeting of the VIVO community to discuss current concerns and deepen cooperation. Topics of concern include improving the ontologies to describe research projects and funding, and mapping initiatives for example with the CERIF community. Anyone interested in research information systems and related ontologies (KDSF, Lattes, PTCris, ROH/Hercules, VIVO) is welcome. The meeting will be held in English. Agenda and meeting notes: [link](https://pad.gwdg.de/vivo_swib23)



</td></tr>




<tr id="P2"><td class="times-col">13:00–19:00</td><td>
### Workshops




</td></tr>


<tr id="contrib111"><td></td><td>

#### Introduction to the Annif automated indexing tool


Osma Suominen [<img src="images/orcid.png" title="ORCID: 0000-0003-0042-0745">](https://orcid.org/0000-0003-0042-0745)<sup>1</sup>, [Mona Lehtinen](speakers.html#X819cc62bd0b0371582dbd601114ec16f627df259b40c0fd1cca07938e8f00be1) [<img src="images/orcid.png" title="ORCID: 0000-0002-4735-0214">](https://orcid.org/0000-0002-4735-0214)<sup>1</sup>, Juho Inkinen [<img src="images/orcid.png" title="ORCID: 0000-0002-6497-6171">](https://orcid.org/0000-0002-6497-6171)<sup>1</sup>, Anna Kasprzik [<img src="images/orcid.png" title="ORCID: 0000-0002-1019-3606">](https://orcid.org/0000-0002-1019-3606)<sup>2</sup>, Lakshmi Bashyam<sup>2</sup>\
<sup>1 </sup>National Library of Finland, Finland; <sup>2 </sup>ZBW – Leibniz Information Centre for Economics, Germany


Many libraries and related institutions are looking at ways of automating their metadata production processes. In this hands-on tutorial, participants will be introduced to the multilingual automated subject indexing tool [Annif](https//annif.org annif.org) as a potential component in a library’s metadata generation system. By completing exercises, participants will get practical experience on setting up Annif, training algorithms using example data, and using Annif to produce subject suggestions for new documents. The exercises cover basic usage as well as more advanced scenarios.

We provide a set of instructional videos and written exercises for the participants to attempt to complete on their own before the tutorial event; see the [Annif-tutorial GitHub repository](https://github.com/NatLibFi/Annif-tutorial/). The actual event will be to a greater extent dedicated to solving problems, asking questions and discussion. Participants are instructed to use a computer with at least 8GB of RAM and at least 20 GB free disk space to complete the exercises. The organizers will provide the software as a preconfigured VirtualBox virtual machine. Alternatively, Docker images and a native Linux install option are provided. No prior experience with the Annif tool is required, but participants are expected to be familiar with subject vocabularies (e.g. thesauri, subject headings or classification systems) and subject metadata that reference those vocabularies.





</td></tr>

<tr id="contrib126"><td></td><td>

#### Network of Terms: bringing links to your data (Workshop)


[Enno Meijers](speakers.html#X7b154e9e21ff33df24142bd521d11e44ec6c913cb01a1b4b4edd50861b713795) [<img src="images/orcid.png" title="ORCID: 0000-0002-2884-3523">](https://orcid.org/0000-0002-2884-3523), Bob Coret [<img src="images/orcid.png" title="ORCID: 0000-0002-7328-1312">](https://orcid.org/0000-0002-7328-1312)\
National Library of the Netherlands / Dutch Digital Heritage Network


How can you find information in heterogeneous Linked Data sources, available at different locations and managed by different owners? At the [Dutch Digital Heritage Network](https://netwerkdigitaalerfgoed.nl/) we developed a federated querying service to solve this problem: [the Network of Terms](https://termennetwerk.netwerkdigitaalerfgoed.nl/). We encourage cultural institutions to publish their data as Linked Data and assign standardized terms to their digital heritage information. Terms are standardized descriptions of concepts or entities that make heritage easier to find for anyone interested in it. Yet it is quite a challenge for institutions to use these terms, because the sources in which the terms are managed – such as thesauri and reference lists – have different API protocols and data models. The Network of Terms removes these barriers. It is a service that searches Linked Data sources in a federative way using the [Comunica framework](https://comunica.dev/). It has a unified SKOS based on GraphQL API that can be easily implemented in collection registration or other systems. It searches the sources for matching terms – real time, with SPARQL. The Network of Terms is published as [open source](https://github.com/netwerk-digitaal-erfgoed/network-of-terms) and its API is already integrated in five commercial systems and one open source system for collection registration. It also provides a Reconcilation API for use in Openrefine and other tools.

Although the core of the software is written in Typescript, the configuration for adding terminology sources through a so-called catalogue requires only basic knowledge of JSON and SPARQL. For testing purposes local instances can be easily set up in Node.js compliant environments. In this workshop we demonstrate the functionality and work with participants to build their specific catalogue using the terminology sources of their preference.



</td></tr>

<tr id="contrib137"><td></td><td>

#### Truly shared cataloguing ecosystem development workshop


[Jason Kovari](speakers.html#Xa39ba4cf9bd02308fb8dbb8875971f9d14b1ef108058011be5c32d2540657d7b) [<img src="images/orcid.png" title="ORCID: 0000-0002-7090-9071">](https://orcid.org/0000-0002-7090-9071)<sup>1</sup>, [Simeon Warner](speakers.html#Xd337b636b1fc07169653c59db94770bb7fade8e975aad6cd48aabff1927d91c9) [<img src="images/orcid.png" title="ORCID: 0000-0002-7970-7855">](https://orcid.org/0000-0002-7970-7855)<sup>1</sup>, Tom Cramer [<img src="images/orcid.png" title="ORCID: 0000-0002-8972-1941">](https://orcid.org/0000-0002-8972-1941)<sup>2</sup>, Philip Schreur<sup>2</sup>\
<sup>1 </sup>Cornell University, United States of America; <sup>2 </sup>Stanford University, United States of America


Current cataloguing practice entails copying data from shared pools into local environments, which enables local editing, but at the cost of data divergence and substantial complexity when trying to aggregate data or perform large-scale enhancement operations. Meanwhile, many BIBFRAME proofs of concept are simply switching MARC for BIBFRAME and thus continuing the practice of copying data. To fulfill the promise of linked data, institutions must stop copying data and instead move to shared source data where groups of institutions consider their data of record to live in stores outside their sole control. This transition is as much a social challenge as it is a technical one.

In this workshop, participants will collaborate to develop the idea of what it means to move cataloguing practice from its current state to one where work is performed in shared data stores, using linked-data approaches rather than copying. Participants will directly engage in structured brainstorming and designing infrastructure components, workflows and metadata issues relevant to shifting to this model. The facilitators will outline some initial thoughts resulting from ten years of BIBFRAME and linked data for cataloguing work, offer these for discussion, and then proceed with several rounds of breakout and discussion. The facilitators will collect notes throughout the workshop and compile a summary report to be placed in an open-access repository soon afterwards.



</td></tr>

<tr id="contrib139"><td></td><td>

#### Creating linked open usable data with Metafacture


[Pascal Christoph](speakers.html#X64176725f870d50355ee0a4a6870702dcd319615603144234f05125ab0649e12), Tobias Bülte, Katinka Tauber\
hbz, Germany


Metafacture is a powerful free and open source ETL tool. We will learn how to transform bibliographic data from MARC21 to JSON-LD (see [link](http://lobid.org/resources/99372425395706441.json) as an example of a result of the ETL from Alma's MARC21 to lobid's JSON-LD). Best practices are shown of how to sensibly do that, i.e. by defining a context and by creating URIs instead of strings for identifiers (IDs). We will enrich our records with SKOS data to gain valuable IDs and Strings. No installation is required – all you need is a browser. 



</td></tr>

<tr id="contrib140"><td></td><td>

#### Nightwatch – metadata management at your fingertips


[Marie-Saphira Flug](speakers.html#X1d181a501799920d70be14d24d158d7f298db7aae105a95a64e0bdc71efbb80f), Lena Klaproth\
Staats- und Universitätsbibliothek Bremen, Germany


In addition to the actual library catalogue, the provision of further specific search engines for a wide variety of search spaces is an increasingly important topic in many libraries. For this purpose, metadata from very different sources in diverse formats must be procured, normalized, and adequately indexed. With a large number of data sources and formats these processes often become confusing, and purely manual processing quickly exceeds the available personnel resources. To automate and monitor the numerous processes for continuously updating metadata as much as possible, we have been developing the internal process management tool "Nightwatch" since 2017. Nightwatch automatically controls all essential phases of the processing of the individual metadata packages in the "life cycle" of the metadata.

In this workshop, participants will create and execute a complete cycle of metadata processing themselves. From the download of metadata from crossref to the conversion and indexing of the metadata. Requirements for this are a laptop with Docker (Compose) installed, preferably with the required images already downloaded, and Python knowledge on beginner level. A repository with all materials is available at [https://gitlab.suub.uni-bremen.de/public-projects/nightwatch-workshop](https://gitlab.suub.uni-bremen.de/public-projects/nightwatch-workshop) and will be updated before the workshop.



</td></tr>


</table>

</div>

<div id="day2">

## DAY 2 | Tuesday, 2023-09-12

<table>

<tr id="S1"><td class="times-col">09:00–10:30</td><td>
### Opening

Moderator: Anna Kasprzik


</td></tr>


<tr id="contrib159"><td></td><td>

#### Welcome


Klaus Tochtermann<sup>1</sup>, Silke Schomburg<sup>2</sup>, Hans-Jörg Lieder<sup>3</sup>\
<sup>1 </sup>Director of ZBW – Leibniz Information Centre for Economics, Germany; <sup>2 </sup>Director of North Rhine-Westphalian Library Service Centre (hbz), Germany; <sup>3 </sup>Berlin State Library – Prussian Cultural Heritage






</td></tr>

<tr id="contrib160"><td></td><td>

#### Keynote: Inherently Social, Decentralized, and for Everyone


[Sarven Capadisli](speakers.html#Xbf39a715ec9f81cd1ba9fc4d78d8865dba9951274a66f890207e34b4d3cb9d37)\
Web Standards Architect


Let's delve into the realms of the lawful, neutral, and chaotic social web. This talk examines the social and technical challenges involved in empowering individuals and communities to assert control over their identity, data, and privacy while having the freedom to choose applications that align with their needs. Specifically, we will explore the advancements in open standards development and deployments in the wild that strive to improve communication and knowledge-sharing for everyone. While there is a plethora of open approaches for the decentralized web, this talk will decipher the underlying philosophies and technical architectures behind initiatives like the [Solid project](https://solidproject.org/) and the [Fediverse](https://en.wikipedia.org/wiki/Fediverse), with some focus on their implications for scholarly communication. We will analyze the possibilities for interoperability among different classes of products in this expansive ecosystem and consider the potential impact of these efforts on libraries, as well as the potential roles libraries can play in this dynamic space.



</td></tr>

<tr id="contrib118"><td></td><td>

#### From ambition to go live: The National Library Board of Singapore’s journey to an operational linked data management & discovery system


[Richard Wallis](speakers.html#X2f59d49c78d886bf95d8e6a8d0d7095973e0e75cb7c2dbf2d7b0cac2c1b5696f)\
Data Liberate, United Kingdom


Like many institutions, the National Library Board (NLB) is responsible to curate, host, and manage many disparate systems across the National Library, national archives and public libraries, both print and digital. The NLB team evolved an ambitious vision for a management and discovery system built upon linked open data and the web, encompassing the many resources they manage. Richard, the project’s linked, structured, and web data, and library metadata consultant, explores the the two-year journey to a live production system.

The agile project between NLB and commercial parters (utilizing a cloud-based environment hosting a semantic graph database, a knowledge graph development & management platform, and server-less compute services) overcame many interesting challenges, including the absence of a single source of truth. The requirements were to provide a continuously updated, reconciled, aggregation of data sources, providing a standardized view of five separate source data systems, each with their individual data formats, data models and curation teams, i.e., a data model capable of supporting a public discovery interface, which required the development of a model that provides a consistent view of all entities regardless of source system. This was constructed using combination of the BIBFRAME and Schema.org vocabularies and involved a data model capable of supporting the consolidated presentation of properties from multiple sources in single primary entities, automatic ingest and reconciliation of daily delta data dumps from source systems, and managing a project team spread across multiple geographies and timezones. There were lessons learnt and practical future plans made, which Richard will also discuss.



</td></tr>




<tr id="CB1"><td class="times-col">10:30–11:00</td><td>
### Coffee Break




</td></tr>





<tr id="S2"><td class="times-col">11:00–12:30</td><td>
### Authorities

Moderator: Joachim Neubert


</td></tr>


<tr id="contrib120"><td></td><td>

#### Supporting sustainable lookup services


[Steven Folsom](speakers.html#X233a86cd1508b481b813efcf1b99f463f69efc4145ecb5e230085146f4767290) [<img src="images/orcid.png" title="ORCID: 0000-0003-3427-5769">](https://orcid.org/0000-0003-3427-5769)\
Cornell University Library, United States of America


As presented at previous SWIB conferences, the [LD4P Authority Lookup Service](https://lookup.ld4l.org/), maintained by Cornell University Library, is designed to be a translation layer that provides an easily consumable normalized API response when searching across multiple authority data sources.

This presentation will reflect on [Linked Data For Production] (https://wiki.lyrasis.org/display/LD4P3)'s recent efforts to more sustainably support lookups in Sinopia (an RDF cataloguing tool, [https://sinopia.io/](https://sinopia.io/)), and share what we have learned about the requirements for robust lookup services. We will describe the opportunities and challenges of relying on caching for lookups, and explain how the significant maintenance costs associated with caching a large number of authorities led us to a no-cache approach, where we instead translate existing authority search APIs into a normalized response. We maintain hope that cache-based lookup services will be feasible for vendors and consortia to support. To this end, and based on the persistent challenge we faced with keeping cached data current, this talk will also describe an [API specification](https://github.com/LD4/entity_metadata_management) we are developing to as a possible way to standardize how data providers communicate changes to their data more frequently than periodic data dumps.



</td></tr>

<tr id="contrib125"><td></td><td>

#### Network of Terms: bringing links to your data


[Enno Meijers](speakers.html#X7b154e9e21ff33df24142bd521d11e44ec6c913cb01a1b4b4edd50861b713795) [<img src="images/orcid.png" title="ORCID: 0000-0002-2884-3523">](https://orcid.org/0000-0002-2884-3523)\
National Library of the Netherlands / Dutch Digital Heritage Network


How can you find information in heterogeneous linked data sources, available at different locations and managed by different owners? At the [Dutch Digital Heritage Network](https://netwerkdigitaalerfgoed.nl/) we developed a federated querying service to solve this problem: [the Network of Terms](https://termennetwerk.netwerkdigitaalerfgoed.nl/). We encourage cultural institutions to publish their data as Linked Data and assign standardized terms to their digital heritage information. Terms are standardized descriptions of concepts or entities that make heritage easier to find for anyone interested in it. Yet it is quite a challenge for institutions to use these terms, because the sources in which the terms are managed – such as thesauri and reference lists – have different API protocols and data models.

The Network of Terms removes these barriers. It is a service that searches linked data sources in a federative way using the [Comunica framework](https://comunica.dev/). It has a unified SKOS based on GraphQL API that can be easily implemented in collection registration or other systems. It searches the sources for matching terms – real time, with SPARQL. The Network of Terms is published as [open source](https://github.com/netwerk-digitaal-erfgoed/network-of-terms) and its API is already integrated in five commercial systems and one open source system for collection registration. It also provides a reconcilation API for use in OpenRefine and other tools. The Network of Terms has been already adopted widely in the cultural heritage domain in the Netherlands but we think it has potential for use in other countries and domains too and we would like to present our tool for the SWIB audience.



</td></tr>

<tr id="contrib106"><td></td><td>

#### Wikibase as an institutional repository authority file


[Joe Cera](speakers.html#Xb31c3e894844fb8071c393c9028ffb2a9a229b622091f40be1f6f0595d50eeac), [Michael Lindsey](speakers.html#X261ceae03d33fc70923efc7b6a4522a733a1fb440cc192154ca12ee03ec39bf0)\
Berkeley Law Library, United States of America


The Berkeley Law Library manages the institutional repository (IR) for Berkeley Law. The IR is part of the institutional TIND integrated library system (ILS). Since these systems are linked through many shared functionalities, we explored various options for creating authority records for the IR that wouldn’t interfere with the ILS authority records file. Prior to this effort, there were no authority records for the IR and all data was added manually which created a great potential for data to be out of sync.

For the past 5 years, we have been using Wikidata QIDs as Berkeley Law faculty identifiers in the IR. In order to maintain consistency, we used the Wikidata API to create an external HTML page used to gather a handful of properties for each QID in the IR which would make manual entry easier and more consistent. In the past 6 months, the IR platform had an authority file update which motivated us to move away from a spreadsheet approach to managing the structure of these records to a more linked data friendly and dynamic method. We created a Wikibase using the wikibase.cloud platform to host this data and explore the potential of using our own Wikibase for internal applications. This instance helps us track data that is more local to our needs while also maintaining connections to other data sources. Entity modification timestamps and revision IDs are used to determine whether or not to initiate an update to our wikibase or to wikidata itself. Updates are limited to a controlled list of properties, such as Library of Congress identifiers, ORCiD, CV, etc. Directionality of particular properties is governed at the application level by the narrative logic of the script.



</td></tr>




<tr id="LU1"><td class="times-col">12:30–14:00</td><td>
### Lunch




</td></tr>





<tr id="S3"><td class="times-col">14:00–15:30</td><td>
### Interactive Session

Moderator: Jakob Voß


</td></tr>


<tr id="contrib162"><td></td><td>

#### Lightning Talks



Use the opportunity to share your latest projects or ideas in a short lightning talk. Talks are registered after the start of the conference.



</td></tr>




<tr id="CB2"><td class="times-col">15:30–16:00</td><td>
### Coffee Break




</td></tr>





<tr id="S4"><td class="times-col">16:00–17:00</td><td>
### Data Modelling

Moderator: Osma Suominen


</td></tr>


<tr id="contrib108"><td></td><td>

#### Hollinger's Box: The retrieval object at the edge of the ontology


Ruth K. Tillman [<img src="images/orcid.png" title="ORCID: 0000-0003-4547-8879">](https://orcid.org/0000-0003-4547-8879)<sup>1</sup>, [Regine Heberlein](speakers.html#Xad5d7f16f7e1fc14edc7713c9829d51c4bdacc00dd0afeb42fcba2913d72063c) [<img src="images/orcid.png" title="ORCID: 0000-0002-0803-8025">](https://orcid.org/0000-0002-0803-8025)<sup>2</sup>\
<sup>1 </sup>Penn State University Libraries, United States of America; <sup>2 </sup>Princeton University Library, United States of America


This presentation examines the extent to which recent and emerging linked data ontologies for archival description allow for a deliberate "contact zone" between intellectual and physical description of archival resources. We define as the contact zone the descriptive space occupied by the retrieval object: the place where the handoff occurs between a discovery system and an administrative system concerned with object management. Archival resources, due to their aggregate and unattested nature, present specific descriptive challenges not shared by other domains within cultural resource description. More often than not, archival resources are represented in intellectual groupings in a way that contextualizes their meaning, while being housed and shelved opportunistically in nested containers in a way that minimizes their footprint, thereby dissociating their physical presence from their intellectual groupings. As a result, the intellectual object identified and requested by a researcher rarely maps one-to-one to the retrieval object stored and paged by a repository.

In this presentation, we examine three recent or emerging linked data standards for archival resources – Records in Contexts, the Art and Rare Materials BIBFRAME extension, and Linked.Art – from the perspective of facilitating retrieval. To what extent do these graphs either engage with the concept of the retrieval object directly or position their edge as a contact zone that may seamlessly integrate with external models defining such an object? How comfortably do the graphs map onto current community practices for handling the ultimate goal of our researchers: getting their hands on the box?



</td></tr>

<tr id="contrib104"><td></td><td>

#### Development of the Share-VDE ontology: goals, principles, and process


Tiziana Possemato<sup>1</sup>, [Jim Hahn](speakers.html#X734432ffbec6a5c0013f2ccd1eb2206543eeb30a22353b7e581a8306b980820b) [<img src="images/orcid.png" title="ORCID: 0000-0001-7924-5294">](https://orcid.org/0000-0001-7924-5294)<sup>2</sup>\
<sup>1 </sup>@Cult/Casalini Libri, Italy; <sup>2 </sup>University of Pennsylvania, United States of America


Share-VDE (SVDE) is a library-driven initiative which brings together the bibliographic catalogues and authority files of a community of libraries in a shared discovery environment based on the linked data ontology BIBFRAME. The SVDE Ontology is an extension to BIBFRAME. The design choices for the SVDE ontology support discovery tasks in the federated linked data environment. This presentation describes the ontology design process, goals, and principles. The overall goals for the SVDE ontology are: 1) the use of the web ontology language (OWL) to publish the classes, properties and constraints that are used in the SVDE environment; 2) to clarify the relationship among Share-VDE entities and other linked data vocabularies and ontologies, and 3) to provide internal (to SVDE) and external (to Library of Congress BIBFRAME) consistency and clarity to classes and properties used in the discovery layer of SVDE. The SVDE ontology is not intended to be a complete departure from BIBFRAME nor is it intended to be an all-new ontology, rather, it is based in BIBFRAME and SVDE is an extension. An overarching design principle is to re-use existing vocabularies wherever possible to reduce complexity of the SVDE ontology. The ontology editing process began by evaluating existing SVDE classes and documenting in OWL; moving next to properties; finally, the process concluded by evaluating any needed restrictions for entities. Entities discussed in this presentation which are novel to SVDE include svde:Opus; svde:Work, and the property svde:hasExpression. The SVDE ontology is interoperable among bibliographic models by using direct references among BIBFRAME, LRM, and RDA entity sets. Interoperability is achieved by asserting that bibliographic entities are described by attribute sets.



</td></tr>




<tr id="DI"><td class="times-col">19:00–21:00</td><td>
### Dinner


Please register for the dinner when you register for the conference.

</td></tr>



</table>

</div>

<div id="day3">

## DAY 3 | Wednesday, 2023-09-13

<table>

<tr id="S5"><td class="times-col">09:00–10:30</td><td>
### Utilizing Wikimedia

Moderator: Katherine Thornton


</td></tr>


<tr id="contrib110"><td></td><td>

#### From EAD to MARC to Wikidata and back again: tools and workflows for archival metadata at the University of Washington libraries


[Crystal Yragui](speakers.html#X78798216429916b486194ae86276d555adaaafc8d658f22d6c6365ea992ac136) [<img src="images/orcid.png" title="ORCID: 0000-0001-5000-282X">](https://orcid.org/0000-0001-5000-282X), [Adam Schiff](speakers.html#Xd6d76cc20adc018ac1cf0c731164740694576aade78fd375c4c04d33189e4ce6) [<img src="images/orcid.png" title="ORCID: 0000-0002-6275-6909">](https://orcid.org/0000-0002-6275-6909)\
University of Washington Libraries, United States of America


The Labor Archives of Washington (LAW) is a collaboration between the University of Washington Libraries (UWL) and the Harry Bridges Center for Labor Studies with support from the local labour community. With over two hundred collections, the archive is one of the largest repositories for labour materials in the United States. Since 2020, UWL has been finding creative ways to integrate traditional library metadata for archival collections into Wikidata. The goal of this work is to better serve researchers by contributing linked open data to Wikidata and to link back from Wikidata to local library metadata.

The presenters will describe semi-automated workflows for the creation of Wikidata items for the people, organizations, and collections in LAW. EAD finding aids are used to generate Wikidata items for agents using Python and OpenRefine. The EAD is also used to generate MARC bibliographic records for the collections, which are then mapped to Wikidata using MarcEdit and OpenRefine. The presenters will also discuss the creation and implementation of a new Wikidata property for Archives West finding aid identifiers and the UWL Wikidata application profile for archival collections. Finally, presenters will share several workflows for interlinking MARC records, library authority records, archival finding aids, Wikipedia articles, and Wikidata items using open source tools such as MarcEdit, OpenRefine, and Quickstatements.



</td></tr>

<tr id="contrib116"><td></td><td>

#### Developing a linked data workflow using Wikidata


[Will Kent](speakers.html#X5b7755a5b3315bc219b87c2815e3021e5bbedb70843ad97f579458d6b9609dab)\
Wiki Education, United States of America


As Wikidata becomes a more popular hub for linked data for libraries and the whole internet, more and more linked data projects use Wikidata. As a decentralized, open source, community-oriented platform, there is no manual for Wikidata work. On one hand this is a relief as it encourages any approach to projects and linked data. On the other hand, it can be overwhelming to start, execute, and finish a project. This presentation will explore some essential takeaways from several project-oriented Wikidata courses.

These courses gather together several library professionals in an effort to create a working community around project-based work. The nature of these projects is diverse – every collection and institution has their own way of setting goals, scoping the work, collection specifics, and how staff can engage with these projects. Drawing on the experience of these participants, we have identified some universal recommendations for realizing Wikidata projects. Embedded in these takeaways are broader themes of linked data education and training around crowdsourcing data models, contributing to community ontologies, and collaboration across collections. Rather than a prescribed set of steps, the conclusions of this presentation are meant to serve as a toolkit for anyone at any point of Wikidata work at their library. In sharing with this community it is our hope we can foster more project work and begin address some of the systemic bias and major content gaps on Wikidata. We also hope to encourage more interest in linked data, its applications, and its adoption into new institutions, tools, and research.



</td></tr>

<tr id="contrib121"><td></td><td>

#### Entity linking historical document OCR by combining Wikidata and Wikipedia


[Kai Labusch](speakers.html#X4c8875ee38504158de79a8ac9e33eb692a33c126da92c58a3aba0c31e95f3ba8) [<img src="images/orcid.png" title="ORCID: 0000-0002-7275-5483">](https://orcid.org/0000-0002-7275-5483), [Clemens Neudecker](speakers.html#X6750b7bcd1a88f24fcc6a05968b2849c949aca2cb2092a51c7d3e2e4099b3e87) [<img src="images/orcid.png" title="ORCID: 0000-0001-5293-8322">](https://orcid.org/0000-0001-5293-8322)\
Berlin State Library, Germany


Named entities like persons, locations and organisations are a prominent target for search in digitized collections. While named entity recognition can be used to automatically detect named entities in texts, through the additional disambiguation and linking of the entities to authority files their usability for retrieval and linking to other sources is significantly improved.

We used Wikidata to construct a comprehensive knowledge-base that holds information on linkable entities and combined it with a Wikipedia-derived corpus of text references that can be used by a neural network-based entity linking system to find references of entities in historical German texts. We demonstrate the feasibility of the approach on ~5,000,000 pages of historical German texts obtained by OCR and show how the entity linking results can be used to group the entire historical text corpus by latent dirichlet allocation. All software components are also released as open source for others to adapt and reuse.



</td></tr>




<tr id="CB3"><td class="times-col">10:30–11:00</td><td>
### Coffee Break




</td></tr>





<tr id="S6"><td class="times-col">11:00–12:30</td><td>
### Collections

Moderator: Jim Hahn


</td></tr>


<tr id="contrib132"><td></td><td>

#### Machine-based subject indexing and beyond for scholarly literature in psychology at ZPID


[Florian A. Grässle](speakers.html#Xea02654253d60681bb567559aef37bee7c53e7bd2dd2e6e570c62d5015758e88) [<img src="images/orcid.png" title="ORCID: 0000-0002-7726-4296">](https://orcid.org/0000-0002-7726-4296), [Tina Trillitzsch](speakers.html#X74ad6699398f9ceb780c2442ff7a32113cc058fab06f56e99911f6aca82f1d13) [<img src="images/orcid.png" title="ORCID: 0000-0001-7239-4844">](https://orcid.org/0000-0001-7239-4844)\
Leibniz Institute for Psychology (ZPID), Germany


PSYNDEX is a reference database for psychological literature from German-speaking countries, growing at a rate of 1,000 new publications per month. The mostly scholarly papers in PSYNDEX are extensively catalogued by human indexers at ZPID (Leibniz Institute for Psychology) along several dimensions and vocabularies specific to psychological research. For the past 15 years, we used a lexical system (AUTINDEX) to generate keyword suggestions for our indexers, based on our vocabulary's main concepts, synonyms and hidden indicators. This system has recently been replaced by the machine-learning based software Annif, and we plan to move to fully automated indexing for part of our records. In this presentation, we will discuss how we integrated Annif into our workflow and most importantly, how we try to assess and improve its suggestions.

Indexers sporadically report specific concepts that Annif failed to suggest (false negatives), or that it wrongly suggested (false positives). We will discuss the "detective work" of classifying these concepts into problem categories and our strategies of dealing with each: e.g. exclusion lists for overly general concepts ("Diagnosis"), boosting new vocabulary concepts not appearing in the training set yet ("COVID-19"), or optimizing the vocabulary itself (like adding more synonyms so lexical parts of the backend can recognize infrequently used concepts). We will also report how we fared with automatically collecting exhaustive lists of such problem concepts by comparing Annif's suggestions with the concepts actually accepted or added by human indexers. Finally, we will present our attempts at going beyond keyword indexing: automatically marking some suggestions as "weighted" (main vs secondary topics) and suggesting publication genre or type, study methodology, and study population.



</td></tr>

<tr id="contrib133"><td></td><td>

#### Implementation of the Albrecht Haupt collection portal based on the general-purpose semantic web application Vitro


Georgy Litvinov [<img src="images/orcid.png" title="ORCID: 0000-0001-5410-5674">](https://orcid.org/0000-0001-5410-5674), [Birte Rubach](speakers.html#Xfffeaa166233421ec71262a37de9a6055da38b132c8f0f2d1cb18f30f0d4c6f3) [<img src="images/orcid.png" title="ORCID: 0000-0001-5319-0221">](https://orcid.org/0000-0001-5319-0221), [Tatiana Walther](speakers.html#Xcc92065d16612fea6c82590c018f47e1bb8316cd6461df6393e44591695a8cc3) [<img src="images/orcid.png" title="ORCID: 0000-0001-8127-2988">](https://orcid.org/0000-0001-8127-2988)\
Technische Informationsbibliothek Hannover, Germany


The Albrecht Haupt Collection is a collection of European drawings and prints, which became publicly available this year on [sah.tib.eu](https://sah.tib.eu/). In this presentation, we describe adoptions of a standard Vitro implemented for indexing environment of an art-historical collection according to the special needs of art-historians on the one hand and linked data principles on the other hand and making it available for further use by experts, broad audience and art-historical Web portals. We also address the technical challenges encountered in the process as well as further developments we are working on.

To describe the items in alignment with standards of cultural data description and export such as the CIDOC CRM and LIDO, the [GESAH Graphic Arts Ontology](https://github.com/tibonto/gesah) was created as an event-centered ontology. Thus, cultural objects are described by a number of activities like e. g. creation or production, agents having various roles in these activities and other attributes e.g. used technique or material. The ontology and collection metadata has been enriched with PIDs from Art & Architecture Thesaurus, ICONCLASS, Getty Thesaurus of Geographic Names, GND and Wikidata. Apart from the specific ontology, new entry forms and display modifications were implemented in an iterative and user-centered approach based on feedback from art historians for recording and representing metadata related to the collection items. We integrated Cantaloupe IIIF to provide instant access to high definition images. Furthermore we developed a highly customizable search that allows users to limit the results area using various filters. A challenge we are currently facing is exporting collection object descriptions to other formats such as LIDO. We will introduce the Dynamic API for the VIVO project with which we aim to solve the aforementioned challenges.



</td></tr>

<tr id="contrib135"><td></td><td>

#### Luhmann as LOD: Transforming and semi-automated mapping keywords to SKOS vocabularies


[Lena-Luise Stahn](speakers.html#X04ec5baae9ca005d4fc310930e8e6effb8dbd093b05c379192a61608eda4a0c6) [<img src="images/orcid.png" title="ORCID: 0000-0003-1298-5145">](https://orcid.org/0000-0003-1298-5145)\
Bergische Universität Wuppertal, Germany


Despite the emphatic demand of producing FAIR data throughout the DH community the phenomenon of data silos is still widespread. Many DH projects set their focus on using standardized formats when putting their data on the web, but disregard the fact that both data and standards have the main purpose in being used by others. This holds true also for the project “Niklas Luhmann – Theory as passion”, a cooperation between the Universities of Bielefeld (Sociology) and Wuppertal (Digital Humanities) aiming at the digitization and indexing of the scientific legacy of Niklas Luhmann (1927–1998) [https://niklas-luhmann-archiv.de/](https://niklas-luhmann-archiv.de/). The digitization of the famous card index (“Zettelkasten”), Luhmann’s manuscripts, and typescripts follows the “traditional” way of transcription, using TEI/XML as data model, and presenting the legacy via the project website, providing the data in XML and JSON format. However, this process does not include transcribing and linking the various authority data existent in notes and scripts. To date the lack of describing named entities constitutes a major gap in the project’s outline.

The presentation is going to address this gap by focussing on Luhmann’s private keyword register. It shows the mapping between TEI and SKOS and its transformation into RDF. Finally it describes the semi-automated process matching the keywords to an external vocabulary (Thesaurus Sozialwissenschaften), using the open source tool AMsterdam ALignment GenerAtion MEtatool (amalgame). With this the presentation will provide a transformation approach of project silo data into LOD, in its technical outline also applicable to projects outside of the sociological discipline.



</td></tr>




<tr id="LU2"><td class="times-col">12:30–14:00</td><td>
### Lunch




</td></tr>





<tr id="S7"><td class="times-col">14:00–15:30</td><td>
### Aggregators

Moderator: Julia Beck


</td></tr>


<tr id="contrib114"><td></td><td>

#### FranceArchives a portal for the French archives


Élodie Thieblin, [Fabien Amarger](speakers.html#X1be14ddfc4ce7c0209a7192bc2a00d8099583cd39a6b1b1b3e8f6b9c68e09b65), Saurfelt Katia\
Logilab


[FranceArchives](https://francearchives.gouv.fr/) is an online aggregator portal designed to provide a single access point to metadata of about 140 French archival institutions. Based on CubicWeb, an open source software, it allows archivists to import archival metadata, publish articles, etc. Researchers, students, genealogy enthusiasts can search above 21 millions of documents. The authorities (people, institutions, places, themes) indexed in the archives are semi-automatically aligned with linked open data repositories (geonames, wikidata, data.culture.fr). This allows disambiguation and cross-referencing of archival data from different sources. The portal information on the authorities is enriched thanks to these alignments. To integrate FranceArchives into the linked open data cloud, the data is also published in RDF, described in the Records In Contexts ontology (RiC-O). [Records in Contexts](https://www.ica.org/en/records-in-contexts-conceptual-model) is a new standard promoted by the International Council on Archives to reconcile the current four standards of description. FranceArchives is one of the first projects to use this standard at such a large scale with success. It is also one of the first archival data repository of this size published as linked open data. A SPARQL endpoint is currently being set up to make the data queryable.



</td></tr>

<tr id="contrib138"><td></td><td>

#### Towards a methodology for validation of metadata enrichments in Europeana


[Antoine Isaac](speakers.html#X8f957d7f7d5690d75cb4a4117e960aa20e007b415eff5524bdb963062f1f627e) [<img src="images/orcid.png" title="ORCID: 0000-0001-9767-6979">](https://orcid.org/0000-0001-9767-6979)<sup>1</sup>, Hugo Manguinhas<sup>1</sup>, Valentine Charles<sup>1</sup>, Monica Marrero [<img src="images/orcid.png" title="ORCID: 0000-0002-2359-6340">](https://orcid.org/0000-0002-2359-6340)<sup>1</sup>, Nuno Freire [<img src="images/orcid.png" title="ORCID: 0000-0002-3632-8046">](https://orcid.org/0000-0002-3632-8046)<sup>1</sup>, Eleftheria Tsoupra<sup>1</sup>, José Grano de Oro<sup>1</sup>, Paolo Scalia<sup>1</sup>, Lianne Heslinga<sup>1</sup>, Alexander Raginsky<sup>2</sup>, Vadim Shestopalov<sup>2</sup>\
<sup>1 </sup>Europeana Foundation, The Netherlands; <sup>2 </sup>Jewish Heritage Network


Metadata enrichment is a powerful way to augment the description of cultural heritage entities, improving data discoverability and supporting end-users in having access to critical context for a given entity. In the Europeana network, several aggregators and projects aim to enrich metadata and/or content for cultural heritage objects that they provide to Europeana. This includes semantic tagging using linked open data vocabularies, translations, transcriptions, etc. A variety of processes are used to produce enrichments, from fully manual (e.g., using crowdsourcing) to fully automatic (e.g., geo-enrichment), and these will vary in terms of quality, reliability and informational value. Therefore, before the enrichments can be integrated into the Europeana services, they must be validated. If the appropriate acceptance criteria are not met, the enrichments should either be rejected or pushed back for improvement.

Europeana is defining a general methodology to assess the overall quality of the enrichments produced by a project or a tool. This methodology does not need to aim at global accept/reject decisions – it can, for example, help to select a subset of the produced enrichments that is deemed trustable enough. The validation efforts may be carried out by members/partners of the network or by the Europeana Foundation – ideally both being involved in some way. Although many projects are prepared so as to include an evaluation effort to assess the quality of the enrichments they produce, Europeana Foundation as the operator of the Europeana service, has the final responsibility for the quality of Europeana's data, therefore it should always be involved in the final vetting. This presentation will introduce the methodology and describe the outcomes of its first application to validate the geo-enrichments provided by the Jewish History Tours project.



</td></tr>

<tr id="contrib131"><td></td><td>

#### The DDB collection and the limits of artificial intelligence


[Mary Ann Tan](speakers.html#X459e0d4873e36a50a21cfefd59555d7e13abbcb285f1b2a44076013a09210307) [<img src="images/orcid.png" title="ORCID: 0000-0003-3634-3550">](https://orcid.org/0000-0003-3634-3550)<sup>1,2</sup>, Harald Sack [<img src="images/orcid.png" title="ORCID: 0000-0001-7069-9804">](https://orcid.org/0000-0001-7069-9804)<sup>1,2</sup>\
<sup>1 </sup>FIZ Karlsruhe, Germany; <sup>2 </sup>Karlsruhe Institute of Technology, Germany


The German Digital Library (DDB) contains a vast collection of over 45 million digitized objects from more than 400 memory institutions across Germany, making it both voluminous and heterogeneous. This presentation includes a discussion of the results of our data analysis on the metadata of digitized objects in the library sector, taking into account the historical background of bibliographic cataloguing, with a particular focus on determining the languages, content, and length of the metadata’s descriptive attributes in the context of the French revolution when the card catalogue was developed. The presentation also covers the results of automatic evaluation of metadata quality, according to metrics such as completeness, accuracy, conformance, consistency, and coherence. This evaluation highlights the extent to which semantic web technologies, namely linked open data and controlled vocabularies, were employed in the DDB. The ultimate goal is to improve the searchability of the DDB collection. With this goal, the presentation includes a demonstration of the advantages of FRBR-ization of DDB’s bibliographic dataset in terms of searchability. This presentation concludes with a discussion of the challenges of the aforementioned proposal, as well as solutions for each of the identified challenges, taking into consideration the recent and rapid developments in the research fields of natural language processing, machine learning, and knowledge graph embeddings.



</td></tr>




<tr id="S8"><td class="times-col">15:30–16:00</td><td>
### Closing

Moderator: Adrian Pohl


</td></tr>





<tr id="CB4"><td class="times-col">16:00–16:30</td><td>
### Farewell Coffee




</td></tr>



</table>

</div>


</div>

<div id="sidebar">

## News {.sidebox-title}

:::{.sidebox-box}

**SWIB23 programme published & registration opened**\
13.07.2023

**CfP published**\
27.02.2023

**SWIB23 – Save the date**\
11.–13. September 2023 in Berlin (Germany)\
02.12.2022

### 14th SWIB

**SWIB22 finished**\
900 registrations from 58 countries\
[Slides and videos are online](../swib22/programme.html) ([Youtube playlist](https://www.youtube.com/playlist?list=PL7fMsenbLiQ2xcclhSnk1nv752E4N_Nqx))\
2.12.2022


:::


## Mastodon {.sidebox-title}

:::{.sidebox-box}

[\@swib@openbiblio.social](https://openbiblio.social/@swib)

#swib23

:::

## Twitter {.sidebox-title}

:::{.sidebox-box}

@swibcon

#swib23

:::

</div>



